---
title: "p8105 HW6 - Sara Kramer (sk4970)"
output: github_document
---

```{r, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(readr)
library(broom)
library(viridis)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

# Problem 2
```{r}
# importing and cleaning data frame. Includes: creating city, state variable, a binary variable if solved or unsolved, fixing an error with one of the city, states, updating variables to appropriate types.
homicide_df = 
  read_csv("data-homicides-master/homicide-data.csv") %>% 
  janitor::clean_names() %>%
  mutate(city_state = as.factor(str_c(city, state, sep = ", ")),
         solved = as.logical(ifelse(disposition %in% c("Closed by arrest"), TRUE, FALSE)),
         reported_date = as.Date(as.character(reported_date),"%Y%m%d"),
         victim_age = as.numeric(victim_age),
         victim_first = str_to_title(victim_first),
         victim_last = str_to_title(victim_last),
         victim_sex = as.factor(victim_sex),
         victim_race = as.factor(victim_race),
         city = as.factor(city),
         state = as.factor(state)) %>%
  filter(city_state != "Dallas, TX" & city_state != "Phoenix, AZ" & city_state != "Kansas City, MO" & city_state != "Tulsa, AL") %>%
  filter(victim_race == "White" | victim_race == "Black")

# glm for baltimore
balt = 
  homicide_df %>%
  filter(city_state ==  "Baltimore, MD")

  balt_glm = glm(solved ~ victim_age + victim_sex + victim_race, data = balt, family = "binomial") 
  
balt_output =
  balt_glm %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) %>%
  filter(term == "victim_sexMale") %>%
  select(term, "adjusted_OR" = "estimate", "CI_lower" = "conf.low", "CI_upper" = "conf.high")

# for all cities
city_glm = 
 homicide_df %>%
 nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(solved ~ victim_age + victim_sex + victim_race, family = "binomial", data = .x)),
    exp_results = map(models, tidy, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95)) %>%
  select(-data, -models) %>% 
  unnest(exp_results) %>%
  filter(term == "victim_sexMale") %>%
  select(city_state, 
         term, 
         "adjusted_OR" = "estimate", 
         "CI_lower" = "conf.low",
         "CI_upper" = "conf.high")
city_glm

# creating a plot to show the estimated ORs and CIs
  city_glm %>%
  ggplot(aes(x = reorder(city_state, +adjusted_OR), y = adjusted_OR)) +
  geom_point(show.legend = FALSE) +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
    title = "Adjusted OR for homicide resolution comparing male victims to female victims",
    x = "City",
    y = "Adjusted OR")
  
  # The plot suggests that after adjusting for victim age and race, homicides where the victim is female has higher odds of being solved compared to homicides with a male victim in many of the cities.
```

# Problem 3
```{r}
# importing and cleaning data frame including making necessary variables character variables and removing missing data
birthweight_df = 
  read_csv("./birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace), 
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(frace), 
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", "puerto rican" = "4")
  ) %>%
  na.omit()
  
  # checking for missing data and anything unusual 
  colSums(is.na(birthweight_df))
  lapply(birthweight_df, unique)
  
  ## there is no missing data but there is a value for menarche = 0 (mother's age), which is odd. 

# modeling
fit = step(lm(bwt ~ ., birthweight_df), direction = "both", trace = FALSE)
tidy(fit)

# residuals v. first model
birthweight_df %>%
  add_residuals(fit) %>%
  add_predictions(fit) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  geom_hline(yintercept = 0,linetype = "dashed")

plot(fit)

# model comparisons - in the first one I look at birth and gestasional age as predictors. In the second I look at interactions with head circumference, length, and sex. 
fit_i = lm(bwt ~ blength + gaweeks, data = birthweight_df)
fit_ii = lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data = birthweight_df)

tidy(fit_i)
tidy(fit_ii)

# comparing models with cross validation
cv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
  )
cv_df = 
  cv_df %>% 
  mutate(
    step_fits = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    i_fits =  map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    ii_fits = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_step = map2_dbl(.x = step_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_i =    map2_dbl(.x = i_fits,    .y = test, ~rmse(model = .x, data = .y)),
    rmse_ii = map2_dbl(.x = ii_fits, .y = test, ~rmse(model = .x, data = .y))
  )

# plotting distributiojns
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()

## based on the plot, the first model (fit) is the best option since it has, on average, the lowest rmse values
```
